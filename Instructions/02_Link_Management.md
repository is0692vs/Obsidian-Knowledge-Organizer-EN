# 🚨 02\_リンク切れ管理

## 最重要原則：リンク切れゼロの維持

### 【絶対遵守】[[]]リンク作成時の必須プロセス

**すべての[[]]リンク作成時に以下を必ず実行：**

```text
【必須手順】
1. 【重要】リンク名にスラッシュが含まれる場合は_に置換
   例：[[CI/CD]] → [[CI_CD]]、[[HTML/CSS]] → [[HTML_CSS]]
   理由：Obsidianがスラッシュをディレクトリ区切りと判定するため
2. file_search ツールで対応ファイルの存在確認
3. 存在しない場合は即座作成完了後に[[]]リンクを追加
4. 作成したファイル内で新たな[[]]リンクを使用する場合は、再度1-3を繰り返す
5. 中間チェック：grep_search でリンク切れがないかチェック
6. 最終確認：semantic_search でリンク切れがないかダブルチェック
```

### 【強化版】リンク切れチェック頻度

**作業プロセス中の必須チェックポイント：**

```text
【リンク切れチェック実行タイミング】
1. 新規ファイル作成直後
2. [[]]リンク追加直後（1つ追加するごと）
3. ファイル編集完了後
4. インデックス更新後
5. 作業セクション完了後
6. 全作業完了前の最終確認（2回実行）

【使用ツール】
- grep_search: "\\[\\[.*\\]\\]" パターンでリンク検索
- file_search: 個別ファイル存在確認
- semantic_search: リンク整合性の総合確認
```

### 【新規追加】リンク切れゼロ保証プロトコル

**すべての作業で以下を厳格遵守：**

```text
【ゼロトレランス・ルール】
1. 【重要】リンク名にスラッシュが含まれる場合は_に置換してからリンク作成
   例：[[CI/CD]] → [[CI_CD]]として処理
2. [[]]リンクを書く前に必ずfile_searchで存在確認
3. 存在しない場合は作業を一時停止してファイル作成
4. 1つのファイルに複数[[]]リンクがある場合、1つずつ順番に確認・作成
5. ファイル作成後は即座にgrep_searchでリンク切れチェック
6. エラーが1つでも検出された場合は全て修正完了まで次の作業禁止

【多段階検証システム】
- 第1段階：file_searchによる個別ファイル確認
- 第2段階：grep_searchによるリンクパターン検索
- 第3段階：semantic_searchによる総合整合性確認
- 第4段階：ファイル構造インデックスとの照合確認
```

## 必須：Python ツールによるリンク切れ検出

**すべてのリンク切れ修復作業は以下の Python スクリプト実行から開始：**

```bash
# 1. リンク切れ一括検出（必須実行）
python link_checker.py

# 出力解釈：
# - "セッション終了条件達成" = リンク切れ0件（作業完了）
# - "セッション終了条件未達成" = リンク切れ有り（修復が必要）
# - "AIエージェント向け：作成すべきファイルリスト (X件)" = 作成すべきファイル一覧
# - 参照回数による優先度表示（多い順に作成推奨）

# 2. 推奨作成順序（参照回数順）
# - OpenAI API (23回) → Words/Tools/OpenAI API.md
# - Azure (10回) → Words/Cloud/Azure.md
# - 自動化 (6回) → Words/Programming/自動化.md
# - スクリーニング (5回) → Words/Finance/スクリーニング.md
```

**【効率化】バッチ作業推奨手順：**

1. **python link_checker.py 実行** → 作成すべきファイル一覧を取得
2. **10-15 件まとめて作成** → 頻出度の高い順から一括作成
3. **表記揺れチェック・統一** → 類似名ファイルの統合・リダイレクト
4. **再度 python link_checker.py 実行** → 進捗確認・残件把握
5. **次のバッチ作成** → セッション終了条件達成まで繰り返し

## バッチ作業プロトコル（10 件/セット）

### Phase 1: 準備・計画

```bash
# 1. 現状把握（必須実行）
python link_checker.py

# 出力から以下を確認：
# - 総リンク切れ件数
# - 優先度別ファイルリスト（参照回数順）
# - 表記揺れ候補
```

### Phase 2: バッチ実行（10 件ずつ）

```text
【1セットの作業範囲】
- 優先度上位10件を選定
- 各ファイルの適切な配置先ディレクトリを決定
- 10件を一括作成（並行作業推奨）
- 作成時に新しい[[]]リンクが生まれた場合は一時リスト化（次セットで対応）

【ファイル作成時の品質基準】
- 最低100字以上の具体的内容
- 2-3個の関連[[]]リンク含有
- カテゴリ適合性の確保
- 基本的なMarkdown構造（見出し、リスト等）
```

### Phase 3: 検証・修正

```bash
# 2. バッチ作成後の検証（必須実行）
python link_checker.py

# 確認項目：
# - リンク切れ件数の減少確認
# - 新規生成された[[]]リンクの把握
# - 表記揺れパターンの検出
```

### Phase 4: 表記揺れ統一

```text
【統一作業の実行】
1. 同一概念の異表記を検出（例：「CI/CD」「CI-CD」「CI CD」）
2. 最適表記を選定（一般性、完全性、可読性で判断）
3. サブファイル作成（リダイレクト記述）
4. メインファイルにエイリアス情報追加
```

**【表記揺れ統一ルール】**

同じ概念の表記違い（例：OpenAI API vs OpenAI-API）は以下で統一：

1. **メインファイル選定**：最も一般的・完全な表記を採用
2. **サブファイル作成**：表記揺れファイルは簡潔なリダイレクト記述

```markdown
# OpenAI-API

[[OpenAI API]]と同義です。OpenAI 社が提供する API サービス。

詳細は[[OpenAI API]]を参照してください。
```

3. **統一原則**：スペース区切り > ハイフン区切り > アンダースコア区切り

### Phase 5: 次セットへの移行

```text
【継続判定】
- リンク切れ0件 → 作業完了
- リンク切れ残存 → 次の10件セットを選定して Phase 2 に戻る
- 表記揺れ検出 → Phase 4 を優先実行

【進捗レポート】
各バッチ完了時に以下を記録：
- 作成ファイル数: X件
- リンク切れ減少: Y件 → Z件
- 表記揺れ統一: N件
- 推定残作業時間: W分
```

## Python ツール利用時の注意点

- ツール実行後も必ず grep_search/semantic_search でダブルチェック実施
- 自動修復後は「ファイル構造インデックス.md」の手動更新必須
- プレースホルダー（[[用語名]]、[[関連用語1]]等）は実際のリンク切れではない
- 具体的なファイル名を持つリンク切れのみ修復対象とする
